import os
import re
import json  
import time
import base64 
import aiohttp
from yarl import URL
import urllib.request
from urllib.parse import urlparse  



# å®šä¹‰URLæœ‰æ•ˆæ€§æ£€æŸ¥å‡½æ•°
def is_valid_url(url):
    parsed = urlparse(url)
    return all([parsed.scheme, parsed.netloc])


def load_json_to_dict(filename):
    """ä»æŒ‡å®šè·¯å¾„åŠ è½½ JSON æ–‡ä»¶å¹¶è¿”å›å­—å…¸ã€‚

    å‚æ•°:
    file_path (str): JSON æ–‡ä»¶çš„è·¯å¾„ã€‚

    è¿”å›:
    dict: åŠ è½½çš„ JSON æ•°æ®å­—å…¸ã€‚
    """
    # è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # æ„é€ å®Œæ•´è·¯å¾„
    file_path = os.path.join(current_dir, filename)
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
        return data
    except FileNotFoundError:
        print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return {}
    except json.JSONDecodeError:
        print(f"æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼: {file_path}")
        return {}


def remove_base64_header(base64_str):
    """
    æ£€æŸ¥å¹¶ç§»é™¤ Base64 å­—ç¬¦ä¸²ä¸­çš„å¤´éƒ¨ä¿¡æ¯ï¼ˆå¦‚ data:image/jpeg;base64,ï¼‰ã€‚
    å¦‚æœä¸å­˜åœ¨å¤´éƒ¨ä¿¡æ¯ï¼Œåˆ™ç›´æ¥è¿”å›åŸå§‹ Base64 å­—ç¬¦ä¸²ã€‚
    """
    # æ­£åˆ™åŒ¹é… Base64 å¤´éƒ¨ä¿¡æ¯ï¼Œå¦‚ data:image/jpeg;base64, æˆ–å…¶ä»–æ ¼å¼
    base64_header_pattern = re.compile(r'^data:image\/[a-zA-Z]+;base64,')
    
    # å¦‚æœåŒ¹é…åˆ°å¤´éƒ¨ä¿¡æ¯ï¼Œåˆ™ç§»é™¤
    if base64_header_pattern.match(base64_str):
        # ä½¿ç”¨æ­£åˆ™å»æ‰å¤´éƒ¨ä¿¡æ¯
        base64_str = base64_header_pattern.sub('', base64_str)
    
    return base64_str

# å®šä¹‰å°†å›¾ç‰‡è½¬æ¢ä¸ºBase64ç¼–ç çš„å‡½æ•°
def image_to_base64(image: bytes) -> str:
    return base64.b64encode(image).decode('utf-8')



async def url_to_base64_and_down(url, root_dir):
    parsed_url = urlparse(url)
    if not parsed_url.scheme.startswith("http"):
        print(f"éæ³•URL: {url}")
        return None

    sub_path = parsed_url.path.lstrip("/")
    local_path = os.path.join(root_dir, sub_path)
    os.makedirs(os.path.dirname(local_path), exist_ok=True)

    if not os.path.exists(local_path):
        start_time = time.perf_counter()
        try:
            async with aiohttp.ClientSession(
                headers={
                    "User-Agent": "Mozilla/5.0",
                    "Accept": "*/*",
                    "Accept-Encoding": "gzip, deflate, br",
                }
            ) as session:
                async with session.get(url, timeout=30) as response:
                    if response.status == 200:
                        content = await response.read()
                        with open(local_path, "wb") as f:
                            f.write(content)
                        print(f"âœ… å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {local_path}ï¼ˆè€—æ—¶ {time.perf_counter() - start_time:.2f}s)")
                    else:
                        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼ˆçŠ¶æ€ç  {response.status}): {url}")
                        return None
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return None
    else:
        print(f"ğŸŸ¡ åŠ è½½ç¼“å­˜å›¾ç‰‡: {local_path}")

    try:
        with open(local_path, "rb") as f:
            return base64.b64encode(f.read()).decode("utf-8")
    except Exception as e:
        print(f"âŒ å›¾ç‰‡è¯»å–å¤±è´¥: {e}")
        return None
        
    
def queue_prompt(prompt, client_id: str, server_address: str, max_retries: int = 4, delay: float = 0.5):
    payload = {"prompt": prompt, "client_id": client_id}
    data = json.dumps(payload).encode("utf-8")
    url = f"http://{server_address}/prompt"

    for attempt in range(1, max_retries + 1):
        try:
            req = urllib.request.Request(url, data=data)
            with urllib.request.urlopen(req) as response:
                raw = response.read()
                content = raw.decode("utf-8").strip()
                

                if not content:
                    raise ValueError("Empty response from server.")
                
                return json.loads(content)
        except Exception as e:
            print(f"[queue_prompt] Attempt {attempt} failed: {e}")
            if attempt < max_retries:
                time.sleep(delay)
            else:
                raise


def sanitize_url_yarl(url: str) -> str:
    """
    ä½¿ç”¨ yarl å®‰å…¨å¤„ç† URL,ç§»é™¤è·¯å¾„ä¸­çš„å¤šä½™ //ï¼Œä¿ç•™å‚æ•°ã€é”šç‚¹ç­‰
    """
    u = URL(url)
    clean_path = re.sub(r'/{2,}', '/', u.path)
    return u.with_path(clean_path).human_repr()